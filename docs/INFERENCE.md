# VMEvalKit Inference Module

## ğŸš€ Quick Start

```bash
# 1. Set up questions directory (only first_frame.png and prompt.txt required)
# questions/chess_task/chess_0000/{first_frame.png, prompt.txt}

# 2. Generate videos (runs all discovered tasks)
python examples/generate_videos.py --questions-dir ./questions --output-dir ./outputs --model svd

# 3. Run with specific models  
python examples/generate_videos.py --questions-dir ./questions --output-dir ./outputs --model luma-ray-2
```

## ğŸ“š Core Concepts

### Task Pairs: The Evaluation Unit

VMEvalKit evaluates video models' reasoning capabilities through **Task Pairs** - carefully designed visual reasoning problems:

| Component | File | Purpose | Required |
|-----------|------|---------|----------|
| ğŸ“¸ **Initial State** | `first_frame.png` | Problem/puzzle to solve | âœ… Required |
| ğŸ“ **Text Prompt** | `prompt.txt` | Natural language instructions | âœ… Required |
| ğŸ¯ **Final State** | `final_frame.png` | Solution/goal reference | âšª Optional |
| ğŸ¬ **Ground Truth** | `ground_truth.mp4` | Reference video | âšª Optional |

**Directory Structure:**
```
questions/
â”œâ”€â”€ chess_task/
â”‚   â”œâ”€â”€ chess_0000/
â”‚   â”‚   â”œâ”€â”€ first_frame.png      # Initial state (required)
â”‚   â”‚   â”œâ”€â”€ prompt.txt           # Instructions (required)
â”‚   â”‚   â”œâ”€â”€ final_frame.png      # Goal state (optional)
â”‚   â”‚   â””â”€â”€ ground_truth.mp4     # Reference (optional)
â”‚   â””â”€â”€ chess_0001/...
â”œâ”€â”€ maze_task/...
â””â”€â”€ sudoku_task/...
```

Models receive the initial state + prompt and must generate videos demonstrating the reasoning process to reach the final state.

## ğŸ—ï¸ Architecture

VMEvalKit uses a **modular architecture** with dynamic loading:

- **MODEL_CATALOG**: Registry of 29+ models across 14 families
- **Dynamic Loading**: Models loaded on-demand via importlib
- **Unified Interface**: All models inherit from `ModelWrapper`
- **Two Categories**:
  - **Commercial APIs**: Instant setup with API keys (Luma, Veo, Sora, Runway)
  - **Open-Source**: Local installation required (LTX-Video, HunyuanVideo, DynamiCrafter, SVD)

## ğŸ“‚ Output Structure

Outputs are organized hierarchically: `model/domain_task/task_id/run_id/`

```
outputs/
â”œâ”€â”€ luma-ray-2/
â”‚   â””â”€â”€ chess_task/
â”‚       â””â”€â”€ chess_0000/
â”‚           â””â”€â”€ luma-ray-2_chess_0000_20250103_143025/
â”‚               â”œâ”€â”€ video/generated_video.mp4
â”‚               â”œâ”€â”€ question/{first_frame.png, prompt.txt, final_frame.png}
â”‚               â””â”€â”€ metadata.json  # Generated: run info, model, duration, status
```



## ğŸ’» Python API

```python
from vmevalkit.runner.inference import InferenceRunner

runner = InferenceRunner(output_dir="./outputs")
result = runner.run(
    model_name="luma-ray-2",
    image_path="questions/chess_task/chess_0000/first_frame.png",
    text_prompt="Find the checkmate move"
)
print(f"Generated: {result['video_path']}")
```


## âš™ï¸ Configuration

### API Keys
```bash
cp env.template .env
# Edit .env with your API keys:
LUMA_API_KEY=your_key_here
OPENAI_API_KEY=your_openai_key
# ... other keys as needed
```