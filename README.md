# VMEvalKit ðŸŽ¥ðŸ§ 

**Unified inference and evaluation framework for 29+ video generation models.**

## Features

- **ðŸš€ 29+ Models**: Unified interface for commercial APIs (Luma, Veo, Sora, Runway) + open-source (LTX-Video, HunyuanVideo, DynamiCrafter, SVD, etc.)
- **âš–ï¸ Evaluation Pipeline**: Human scoring (Gradio) + automated scoring (GPT-4O, InternVL)  
- **â˜ï¸ Cloud Integration**: S3 + HuggingFace Hub support

## Data Format

Organize your questions outside VMEvalKit with the following structure:

```
questions/
â””â”€â”€ {domain}_task/                    # task folder (e.g., chess_task, matching_object_task)
    â”œâ”€â”€ {domain}_0000/                # individual question folder
    â”‚   â”œâ”€â”€ first_frame.png           # required: input image for video generation
    â”‚   â”œâ”€â”€ prompt.txt                # required: text prompt describing the video
    â”‚   â”œâ”€â”€ final_frame.png           # optional: expected final frame for evaluation
    â”‚   â””â”€â”€ ground_truth.mp4          # optional: reference video for evaluation
    â”œâ”€â”€ {domain}_0001/
    â”‚   â””â”€â”€ ...
    â””â”€â”€ {domain}_0002/
        â””â”€â”€ ...
```

**Example** with domain `chess`:
```
questions/
â””â”€â”€ chess_task/
    â”œâ”€â”€ chess_0000/
    â”‚   â”œâ”€â”€ first_frame.png
    â”‚   â”œâ”€â”€ prompt.txt
    â”‚   â”œâ”€â”€ final_frame.png
    â”‚   â””â”€â”€ ground_truth.mp4
    â”œâ”€â”€ chess_0001/
    â”‚   â””â”€â”€ ...
    â””â”€â”€ chess_0002/
        â””â”€â”€ ...
```

**Naming Convention:**
- **Task folder**: `{domain}_task` (e.g., `chess_task`, `matching_object_task`)
- **Question folders**: `{domain}_{i:04d}` where `i` is zero-padded (e.g., `chess_0000`, `chess_0064`). Padding automatically expands beyond 4 digits when neededâ€”no dataset size limit.

## Quick Start

```bash
# 1. Install
git clone https://github.com/Video-Reason/VMEvalKit.git
cd VMEvalKit

python -m venv venv
source venv/bin/activate

pip install -e .

# 2. Setup models
bash setup/install_model.sh --model svd --validate

# # 3. Organize your questions data (see format above)
# mkdir -p ~/my_research/questions

# 4. Run inference
python examples/generate_videos.py --questions-dir setup/test_assets/ --output-dir ./outputs --model svd
python examples/generate_videos.py --questions-dir setup/test_assets/ --output-dir ./outputs --model LTX-2
# 5. Run evaluation  
# Create eval_config.json first:
echo '{"method": "human", "inference_dir": "~/my_research/outputs", "eval_output_dir": "~/my_research/evaluations"}' > eval_config.json
python examples/score_videos.py --eval-config eval_config.json
```

## API Keys

Set in `.env` file:
```bash
cp env.template .env
# Edit .env with your API keys:
# LUMA_API_KEY=...
# OPENAI_API_KEY=...  
# GEMINI_API_KEY=...
```

## Adding Models

```python
# Inherit from ModelWrapper
from vmevalkit.models.base import ModelWrapper

class MyModelWrapper(ModelWrapper):
    def generate(self, image_path, text_prompt, **kwargs):
        # Your inference logic
        return {"success": True, "video_path": "...", ...}
```

Register in `vmevalkit/runner/MODEL_CATALOG.py`:
```python
"my-model": {
    "wrapper_module": "vmevalkit.models.my_model_inference",
    "wrapper_class": "MyModelWrapper", 
    "family": "MyCompany"
}
```

## License

Apache 2.0